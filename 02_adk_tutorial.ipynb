{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_1_'></a>[ADK Features](#toc0_)\n",
        "\n",
        "- This notebook contains the key features of ADK that can help you build your custom agent.\n",
        "- Credit to lavinigam@"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [ADK Features](#toc1_1_)    \n",
        "  - [LLM Agent with a Single Tool](#toc1_2_)    \n",
        "  - [LLM Agent with a Input/Output Schema](#toc1_3_)    \n",
        "  - [LLM Agent with a output_key](#toc1_4_)    \n",
        "  - [LLM Agent with a built_in_code_execution](#toc1_5_)    \n",
        "  - [LLM Agent with a Multiple Tool](#toc1_6_)    \n",
        "  - [LLM Agent with single sub-agent](#toc1_7_)    \n",
        "  - [LLM Agents with Callbacks (Agent, Model & Tool)](#toc1_8_)    \n",
        "  - [LLM Agent with before_agent_callback and state](#toc1_9_)    \n",
        "  - [LLM Agent with after_agent_callback and state](#toc1_10_)    \n",
        "  - [LLM Agent with before_model_callback and state](#toc1_11_)    \n",
        "  - [LLM Agent with after_model_callback and state](#toc1_12_)    \n",
        "  - [LLM Agent with before_tool_callback and state](#toc1_13_)    \n",
        "  - [LLM Agent with after_tool_callback and state](#toc1_14_)    \n",
        "  - [LLM Agent with Gaurdrail (Profanity Checker with before_model callback)](#toc1_15_)    \n",
        "  - [LlmAgent with All Callbacks use case](#toc1_16_)    \n",
        "  - [Session Service](#toc1_17_)    \n",
        "  - [Session State - State Manupilation](#toc1_18_)    \n",
        "  - [Session State - delta_states](#toc1_19_)    \n",
        "  - [Accessing Session Properties](#toc1_20_)    \n",
        "  - [InMemory Session Service](#toc1_21_)    \n",
        "  - [Database Session Service (with SQLite for demo)](#toc1_22_)    \n",
        "  - [LlmAgent with Anthropic (3rd Party Model)](#toc1_23_)    \n",
        "  - [Artifact Service](#toc1_24_)      \n",
        "  - [Sequence Agent](#toc1_25_)      \n",
        "  - [Loop Agent](#toc1_26_)      \n",
        "  - [Parallel Agent](#toc1_27_)      \n",
        "  - [Custom Agent](#toc1_28_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version: 1.0.0\n",
            "Version: 1.93.1\n",
            "Version: 1.16.1\n"
          ]
        }
      ],
      "source": [
        "!uv pip show google-adk google-genai google-cloud-aiplatform | grep Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Temp - to supress WARNING:google_genai.types:\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=UserWarning, module=\"google.generativeai.types.content_types\"\n",
        ")  # Suppress harmless warning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:red\">**YOU NEED TO UPDATE YOUR PROJECT_ID AND LOCATION**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"hello-world-418507\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"  # Use Vertex AI API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_2_'></a>[LLM Agent with a Single Tool](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The capital of France is Paris.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel\n",
        "from google.adk.agents import Agent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_city_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-exp\"\n",
        "\n",
        "\n",
        "# Define a simple tool\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\n",
        "\n",
        "    Args:\n",
        "        country: The name of the country.\n",
        "\n",
        "    Returns:\n",
        "        The capital city of the country.\n",
        "    \"\"\"\n",
        "    country_capitals = {\n",
        "        \"united states\": \"washington, d.c.\",\n",
        "        \"canada\": \"ottawa\",\n",
        "        \"france\": \"paris\",\n",
        "    }\n",
        "    return country_capitals.get(country.lower(), \"Capital not found\")\n",
        "\n",
        "\n",
        "# Agent\n",
        "capital_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"capital_agent\",\n",
        "    description=\"An agent that can retrieve the capital city of a country.\",\n",
        "    instruction=\"\"\"You are an agent that can retrieve the capital city of a country.\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, use the `get_capital_city` tool to retrieve the capital city for that country.\n",
        "    Finally, present the capital city to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    tools=[get_capital_city],\n",
        "    generate_content_config=types.GenerateContentConfig(\n",
        "        max_output_tokens=100,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    async for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "await call_agent(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_3_'></a>[LLM Agent with a Input/Output Schema](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  {\n",
            "\"capital\": \"Paris\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from pydantic import Field\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "class InputSchema(BaseModel):\n",
        "    country: str = Field(description=\"The country to find the capital of.\")\n",
        "\n",
        "\n",
        "class OutputSchema(BaseModel):\n",
        "    capital: str = Field(description=\"The capital of the country.\")\n",
        "\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given country.\n",
        "\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, respond with the capital of that country in the following JSON format:\n",
        "\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a country.\"\"\",\n",
        "    disallow_transfer_to_peers=True,\n",
        "    disallow_transfer_to_parent=True,\n",
        "    input_schema=InputSchema,\n",
        "    output_schema=OutputSchema,\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    async for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "await call_agent('{\"country\": \"France\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_4_'></a>[LLM Agent with a output_key](#toc0_)\n",
        "\n",
        "- You set `output_key` to store the key value pair into session state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  France: Paris\n",
            "\n",
            "Session State: France: Paris\n",
            "\n",
            "Agent Response:  Germany: Berlin\n",
            "\n",
            "Session State: Germany: Berlin\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import json\n",
        "\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given country.\n",
        "\n",
        "    When a user provides a prompt, extract the country name.\n",
        "\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a country.\"\"\",\n",
        "    output_key=\"capital_output\",\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    async for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "            temp = await session_service.get_session(   \n",
        "                app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        "            )\n",
        "            print(\n",
        "                \"Session State:\",\n",
        "                temp.state.get(\"capital_output\"),\n",
        "            )\n",
        "\n",
        "\n",
        "await call_agent(\"What's the capital of France?\")\n",
        "await call_agent(\"What's the capital of Germany?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_5_'></a>[LLM Agent with a built_in_code_execution](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['executable_code', 'code_execution_result'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Agent Response ---\n",
            "111 + 222 = 333\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import json\n",
        "from google.adk.code_executors import BuiltInCodeExecutor\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash\"\n",
        "\n",
        "code_agent = LlmAgent(\n",
        "    name=\"code_execution_agent\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    code_executor=BuiltInCodeExecutor(),\n",
        "    instruction=\"Generate python code to solve the user's request. \"\n",
        "    \"If the user asks for a specific output, return the output of the code execution.\",\n",
        ")\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=code_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    async for event in events:\n",
        "        if event.content and event.content.parts:\n",
        "            for part in event.content.parts:\n",
        "                if part.text:\n",
        "                    print(f\"\\n--- Agent Response ---\\n{part.text}\\n\")\n",
        "        else:\n",
        "            print(f\"Event: {event}\")\n",
        "\n",
        "\n",
        "await call_agent(\"what is 111 + 222\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_6_'></a>[LLM Agent with a Multiple Tool](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  I found one recipe with chicken: chicken tikka masala.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  No, pasta carbonara is not suitable for a vegan diet.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The ingredients for vegan lentil soup are: lentils, carrots, celery, onion, vegetable broth.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "# Constants\n",
        "APP_NAME = \"recipe_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"recipe_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# --- Mock Data ---\n",
        "recipes = {\n",
        "    \"pasta carbonara\": {\n",
        "        \"ingredients\": [\n",
        "            \"pasta\",\n",
        "            \"eggs\",\n",
        "            \"guanciale\",\n",
        "            \"pecorino romano\",\n",
        "            \"black pepper\",\n",
        "        ],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"chicken tikka masala\": {\n",
        "        \"ingredients\": [\"chicken\", \"yogurt\", \"ginger\", \"garlic\", \"masala blend\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"vegan lentil soup\": {\n",
        "        \"ingredients\": [\"lentils\", \"carrots\", \"celery\", \"onion\", \"vegetable broth\"],\n",
        "        \"dietary_restrictions\": [\"vegan\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# --- Tools ---\n",
        "def search_recipes(keyword: str) -> str:\n",
        "    \"\"\"Searches for recipes based on a keyword.\n",
        "\n",
        "    Args:\n",
        "        keyword: The keyword to search for (e.g., ingredient).\n",
        "\n",
        "    Returns:\n",
        "        A string containing the names of matching recipes, or a message if no recipes are found.\n",
        "    \"\"\"\n",
        "    matching_recipes = [\n",
        "        recipe_name\n",
        "        for recipe_name, recipe_data in recipes.items()\n",
        "        if keyword.lower() in recipe_name.lower()\n",
        "        or keyword.lower() in recipe_data[\"ingredients\"]\n",
        "    ]\n",
        "    if matching_recipes:\n",
        "        return f\"Recipes matching '{keyword}': {', '.join(matching_recipes)}.\"\n",
        "    else:\n",
        "        return f\"No recipes found matching '{keyword}'.\"\n",
        "\n",
        "\n",
        "def check_dietary_restrictions(recipe_name: str, dietary_restriction: str) -> str:\n",
        "    \"\"\"Checks if a recipe is suitable for a given dietary restriction.\n",
        "\n",
        "    Args:\n",
        "        recipe_name: The name of the recipe to check.\n",
        "        dietary_restriction: The dietary restriction to check for (e.g., \"vegan\").\n",
        "\n",
        "    Returns:\n",
        "        A string indicating if the recipe is suitable or not.\n",
        "    \"\"\"\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        if dietary_restriction.lower() in recipe_data[\"dietary_restrictions\"]:\n",
        "            return f\"'{recipe_name}' is suitable for a '{dietary_restriction}' diet.\"\n",
        "        else:\n",
        "            return (\n",
        "                f\"'{recipe_name}' is not suitable for a '{dietary_restriction}' diet.\"\n",
        "            )\n",
        "    else:\n",
        "        return f\"Recipe '{recipe_name}' not found.\"\n",
        "\n",
        "\n",
        "def get_ingredient_list(recipe_name: str) -> str:\n",
        "    \"\"\"Returns a list of ingredients for a given recipe.\n",
        "\n",
        "    Args:\n",
        "        recipe_name: The name of the recipe.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the list of ingredients, or a message if the recipe is not found.\n",
        "    \"\"\"\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        return (\n",
        "            f\"Ingredients for '{recipe_name}': {', '.join(recipe_data['ingredients'])}.\"\n",
        "        )\n",
        "    else:\n",
        "        return f\"Recipe '{recipe_name}' not found.\"\n",
        "\n",
        "\n",
        "# --- Agent ---\n",
        "recipe_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Recipe Agent. Your task is to help users find recipes and check their suitability for dietary restrictions.\n",
        "\n",
        "    You have access to three tools:\n",
        "    1. `search_recipes`: Use this tool to find recipes based on a keyword (e.g., ingredient).\n",
        "    2. `check_dietary_restrictions`: Use this tool to check if a recipe is suitable for a given dietary restriction.\n",
        "    3. `get_ingredient_list`: Use this tool to get a list of ingredients for a given recipe.\n",
        "\n",
        "    When a user provides a prompt, first determine what they are asking for.\n",
        "    - If they are asking for recipes based on a keyword, use the `search_recipes` tool.\n",
        "    - If they are asking if a recipe is suitable for a dietary restriction, use the `check_dietary_restrictions` tool.\n",
        "    - If they are asking for a list of ingredients for a recipe, use the `get_ingredient_list` tool.\n",
        "    Finally, present the information to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    description=\"\"\"An agent that can find recipes, check dietary restrictions, and list ingredients.\n",
        "    It has access to the `search_recipes`, `check_dietary_restrictions`, and `get_ingredient_list` tools.\"\"\",\n",
        "    tools=[\n",
        "        search_recipes,\n",
        "        check_dietary_restrictions,\n",
        "        get_ingredient_list,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# --- Session and Runner ---\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=recipe_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# --- Agent Interaction ---\n",
        "async def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    async for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "await call_agent(\"Find recipes with chicken.\")\n",
        "await call_agent(\"Is pasta carbonara suitable for a vegan diet?\")\n",
        "await call_agent(\"What are the ingredients in vegan lentil soup?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_7_'></a>[LLM Agent with single sub-agent](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "U9RE0gb0TFkg"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import asyncio\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        \"chicago\": {\"temperature\": 25, \"condition\": \"sunny\", \"sky\": \"clear\"},\n",
        "        \"toronto\": {\"temperature\": 30, \"condition\": \"partly cloudy\", \"sky\": \"overcast\"},\n",
        "        \"chennai\": {\"temperature\": 15, \"condition\": \"rainy\", \"sky\": \"cloudy\"},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "def get_greeting(name: str) -> str:\n",
        "    \"\"\"Greets the given name.\n",
        "\n",
        "    Args:\n",
        "        name: The name to greet.\n",
        "\n",
        "    Returns:\n",
        "        A greeting message.\n",
        "    \"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "# Agent\n",
        "root_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"\"\"You are a helpful agent with tool and sub-agents.\n",
        "    - When user ask about weather, extract the city name, then use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    - If the user asks for a greeting, transfer to the greeting_agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city, and also greet a user.\"\"\",\n",
        "    tools=[get_weather],\n",
        "    # allow_transfer=True,\n",
        ")\n",
        "\n",
        "greeting_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"greeting_agent\",\n",
        "    instruction=\"\"\"You are a Greeting Agent. Your task is to greet the user.\n",
        "\n",
        "    When a user provides a prompt, extract the name.\n",
        "    Then, use the `get_greeting` tool to greet the user.\n",
        "    Finally, present the greeting to the user in a clear and concise manner.\n",
        "    If the user asks for weather information, transfer to the weather agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can greet a user.\n",
        "    You have access to the `get_greeting` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_greeting],\n",
        "    disallow_transfer_to_parent=True,\n",
        "    disallow_transfer_to_peers=True,\n",
        ")\n",
        "\n",
        "# Set parent-child relationship\n",
        "root_agent.sub_agents = [greeting_agent]\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    ):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_70TiFhAfP3Z",
        "outputId": "9cfeb22e-a07d-4bf0-91e0-182a662b6699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in toronto is 30 degrees Celsius, partly cloudy with a overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7YvXK5afP0C",
        "outputId": "0411cab1-c4fe-40ea-a5c2-e49b17758513"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Hello, Jimmy!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"Hello, Jimmy!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in chennai is 15 degrees Celsius, rainy with a cloudy sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in chennai?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_8_'></a>[LLM Agents with Callbacks (Agent, Model & Tool)](#toc0_)\n",
        "\n",
        "- This is useful when you want to inspect the messege exchange agents\n",
        "- This cell shows the available functio parameters that you can use in callback\n",
        "\n",
        "\n",
        "This is how the sequence looks like:\n",
        "- Before Agent Callback > Before Model Callback > After Model Callback > Before Tool Callback > After Tool Callback > Before Model Callback > After Model Callback > After Agent Callback\n",
        "\n",
        "![callback](assets/callback.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "wPVFcZ84sfLe"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import asyncio\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        \"chicago\": {\"temperature\": 25, \"condition\": \"sunny\", \"sky\": \"clear\"},\n",
        "        \"toronto\": {\"temperature\": 30, \"condition\": \"partly cloudy\", \"sky\": \"overcast\"},\n",
        "        \"chennai\": {\"temperature\": 15, \"condition\": \"rainy\", \"sky\": \"cloudy\"},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "def get_greeting(name: str) -> str:\n",
        "    \"\"\"Greets the given name.\n",
        "\n",
        "    Args:\n",
        "        name: The name to greet.\n",
        "\n",
        "    Returns:\n",
        "        A greeting message.\n",
        "    \"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "\n",
        "def before_model_callback(callback_context, llm_request):\n",
        "    print(\n",
        "        f\"Before Model Callback: Agent {callback_context._invocation_context.agent.name}, Request: {llm_request.contents}\"\n",
        "    )\n",
        "    return None\n",
        "\n",
        "\n",
        "def after_model_callback(callback_context, llm_response):\n",
        "    print(\n",
        "        f\"After Model Callback: Agent {callback_context._invocation_context.agent.name}, Response: {llm_response.content}\"\n",
        "    )\n",
        "    return None\n",
        "\n",
        "\n",
        "def before_tool_callback(tool, args, tool_context):\n",
        "    print(f\"Before Tool Callback: Tool {tool.name}, Args: {args}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def after_tool_callback(tool, args, tool_context, tool_response):\n",
        "    print(f\"After Tool Callback: Tool {tool.name}, Response: {tool_response}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def before_agent_callback(callback_context):\n",
        "    print(\n",
        "        f\"Before Agent Callback: Agent {callback_context._invocation_context.agent.name}\"\n",
        "    )\n",
        "    return None\n",
        "\n",
        "\n",
        "def after_agent_callback(callback_context):\n",
        "    print(\n",
        "        f\"After Agent Callback: Agent {callback_context._invocation_context.agent.name}\"\n",
        "    )\n",
        "    return None\n",
        "\n",
        "\n",
        "root_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"weather_agent\",\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\n",
        "    If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        "    before_model_callback=before_model_callback,\n",
        "    after_model_callback=after_model_callback,\n",
        "    before_tool_callback=before_tool_callback,\n",
        "    after_tool_callback=after_tool_callback,\n",
        "    before_agent_callback=before_agent_callback,\n",
        "    after_agent_callback=after_agent_callback,\n",
        ")\n",
        "\n",
        "greeting_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"greeting_agent\",\n",
        "    instruction=\"\"\"You are a Greeting Agent. Your task is to greet the user.\n",
        "\n",
        "    When a user provides a prompt, extract the name.\n",
        "    Then, use the `get_greeting` tool to greet the user.\n",
        "    Finally, present the greeting to the user in a clear and concise manner.\n",
        "    If the user asks for weather information, transfer to the weather agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can greet a user.\n",
        "    You have access to the `get_greeting` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_greeting],\n",
        "    disallow_transfer_to_parent=True,\n",
        "    disallow_transfer_to_peers=True,\n",
        "    before_model_callback=before_model_callback,\n",
        "    after_model_callback=after_model_callback,\n",
        "    before_tool_callback=before_tool_callback,\n",
        "    after_tool_callback=after_tool_callback,\n",
        "    before_agent_callback=before_agent_callback,\n",
        "    after_agent_callback=after_agent_callback,\n",
        ")\n",
        "\n",
        "# Set parent-child relationship\n",
        "root_agent.sub_agents = [greeting_agent]\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    ):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Agent Callback: Agent weather_agent\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text=\"What's the weather in Chicago?\")], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool get_weather, Args: {'city': 'Chicago'}\n",
            "After Tool Callback: Tool get_weather, Response: Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(will_continue=None, scheduling=None, id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), text=None)], role='user')]\n",
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')] role='model'\n",
            "Agent Response:  The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "\n",
            "After Agent Callback: Agent weather_agent\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_9_'></a>[LLM Agent with before_agent_callback and state](#toc0_)\n",
        "\n",
        "- In this example, we check the session state `skip_agent`, and decide how the agent will response.\n",
        "- If condition is met, we skip LLM response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally ---\n",
            "[Callback] Entering agent: SimpleLlmAgent (Invocation: e-7343f914-a171-4276-ba13-2b10874359a3)\n",
            "[Callback] Condition not met: Proceeding with agent SimpleLlmAgent.\n",
            "Event Output: SimpleLlmAgent: Hello!\n",
            "\n",
            "--- Running LLM Agent with Skip Condition ---\n",
            "[Callback] Entering agent: SimpleLlmAgent (Invocation: e-fc84f785-e640-4ae9-80c9-e754851b056a)\n",
            "[Callback] Condition met: Skipping agent SimpleLlmAgent.\n",
            "Event Output: SimpleLlmAgent: Agent SimpleLlmAgent was skipped by callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred model\n",
        "\n",
        "\n",
        "# --- Define the Callback Function (Same as before) ---\n",
        "def simple_before_agent_logger(\n",
        "    callback_context: CallbackContext,\n",
        ") -> Optional[types.Content]:\n",
        "    \"\"\"Logs entry into an agent and checks a condition.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    invocation_id = callback_context.invocation_id\n",
        "    print(f\"[Callback] Entering agent: {agent_name} (Invocation: {invocation_id})\")\n",
        "\n",
        "    # Example: Check a condition in state\n",
        "    if callback_context.state.get(\"skip_agent\", False):\n",
        "        print(f\"[Callback] Condition met: Skipping agent {agent_name}.\")\n",
        "        # Return Content to skip the agent's run\n",
        "        return types.Content(\n",
        "            parts=[types.Part(text=f\"Agent {agent_name} was skipped by callback.\")]\n",
        "        )\n",
        "    else:\n",
        "        print(f\"[Callback] Condition not met: Proceeding with agent {agent_name}.\")\n",
        "        # Return None to allow the agent's run to execute\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"SimpleLlmAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a simple agent. Just say 'Hello!'\",\n",
        "        description=\"An LLM agent demonstrating before_agent_callback\",\n",
        "        before_agent_callback=simple_before_agent_logger,\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(\n",
        "        agent=my_llm_agent, app_name=\"llm_demo_app\", session_service=session_service\n",
        "    )\n",
        "    session_id_run = \"llm_session_run_1\"\n",
        "    session_id_skip = \"llm_session_skip_1\"\n",
        "    user_id = \"llm_test_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    await session_service.create_session(\n",
        "        app_name=\"llm_demo_app\", user_id=user_id, session_id=session_id_run\n",
        "    )\n",
        "    await session_service.create_session(\n",
        "        app_name=\"llm_demo_app\",\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_skip,\n",
        "        state={\"skip_agent\": True},\n",
        "    )  # Set state to trigger skip condition\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_run,\n",
        "        new_message=types.Content(role=\"user\", parts=[types.Part(text=\"Run normally\")]),\n",
        "    ):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )  # Added strip() for cleaner output\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Skip Condition ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_skip,\n",
        "        new_message=types.Content(parts=[types.Part(text=\"Skip this agent\")]),\n",
        "    ):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_10_'></a>[LLM Agent with after_agent_callback and state](#toc0_)\n",
        "\n",
        "- In this use case, we check the `add_concluding_note` session state.\n",
        "- If condition is met, we append extra message to LLM response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred model\n",
        "\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_agent_logger(\n",
        "    callback_context: CallbackContext,\n",
        ") -> Optional[types.Content]:\n",
        "    \"\"\"Logs exit from an agent and optionally appends a message.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    invocation_id = callback_context.invocation_id\n",
        "    print(f\"[Callback] Exiting agent: {agent_name} (Invocation: {invocation_id})\")\n",
        "\n",
        "    # Example: Optionally return Content to append a message\n",
        "    if callback_context.state.get(\"add_concluding_note\", False):\n",
        "        print(f\"[Callback] Adding concluding note for agent {agent_name}.\")\n",
        "        # Return Content to append after the agent's own output\n",
        "        return types.Content(\n",
        "            parts=[types.Part(text=f\"Concluding note added by after_agent_callback.\")]\n",
        "        )\n",
        "    else:\n",
        "        print(f\"[Callback] No concluding note added for agent {agent_name}.\")\n",
        "        # Return None - no additional message appended\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"SimpleLlmAgentWithAfter\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a simple agent. Just say 'Processing complete!'\",\n",
        "        description=\"An LLM agent demonstrating after_agent_callback\",\n",
        "        after_agent_callback=simple_after_agent_logger,  # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(\n",
        "        agent=my_llm_agent,\n",
        "        app_name=\"llm_demo_app_after\",\n",
        "        session_service=session_service,\n",
        "    )\n",
        "    session_id_run = \"llm_session_run_after_1\"\n",
        "    session_id_conclude = \"llm_session_conclude_1\"\n",
        "    user_id = \"llm_test_user_after\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_demo_app_after\", user_id=user_id, session_id=session_id_run\n",
        "    )\n",
        "    # Session where the callback will add a note\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_demo_app_after\",\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_conclude,\n",
        "        state={\"add_concluding_note\": True},\n",
        "    )\n",
        "\n",
        "    # Test with different session_id\n",
        "    print(\"--- Running LLM Agent Normally (with after_agent_callback) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_run,\n",
        "        new_message=types.Content(role=\"user\", parts=[types.Part(text=\"Run normally\")]),\n",
        "    ):\n",
        "        # Print any event content from agent or callback\n",
        "        if event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Concluding Note Condition ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_conclude,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"Run and conclude\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Print any event content from agent or callback\n",
        "        if event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_11_'></a>[LLM Agent with before_model_callback and state](#toc0_)\n",
        "\n",
        "In this example, we demonstrate how to\n",
        "\n",
        "- modify system instruction\n",
        "- block model response based on keyword in query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "\n",
        "# Need LlmRequest and LlmResponse for the callback signature and return type\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred model\n",
        "\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_before_model_modifier(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Inspects/modifies the LLM request or skips the call.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    print(f\"[Callback] Before model call for agent: {agent_name}\")\n",
        "\n",
        "    # Inspect the last user message in the request contents\n",
        "    last_user_message = \"\"\n",
        "    if llm_request.contents and llm_request.contents[-1].role == \"user\":\n",
        "        if llm_request.contents[-1].parts:\n",
        "            last_user_message = llm_request.contents[-1].parts[0].text\n",
        "    print(f\"[Callback] Inspecting last user message: '{last_user_message}'\")\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # Add a prefix to the system instruction\n",
        "    original_instruction = llm_request.config.system_instruction or types.Content(\n",
        "        role=\"system\", parts=[]\n",
        "    )\n",
        "    prefix = \"[Modified by Callback] \"\n",
        "    # Ensure system_instruction is Content and parts list exists\n",
        "    if not isinstance(original_instruction, types.Content):\n",
        "        # Handle case where it might be a string (though config expects Content)\n",
        "        original_instruction = types.Content(\n",
        "            role=\"system\", parts=[types.Part(text=str(original_instruction))]\n",
        "        )\n",
        "    if not original_instruction.parts:\n",
        "        original_instruction.parts.append(\n",
        "            types.Part(text=\"\")\n",
        "        )  # Add an empty part if none exist\n",
        "\n",
        "    # Modify the text of the first part\n",
        "    modified_text = prefix + (original_instruction.parts[0].text or \"\")\n",
        "    original_instruction.parts[0].text = modified_text\n",
        "    llm_request.config.system_instruction = original_instruction\n",
        "    print(f\"[Callback] Modified system instruction to: '{modified_text}'\")\n",
        "\n",
        "    # --- Skip Example ---\n",
        "    # Check if the last user message contains \"BLOCK\"\n",
        "    if \"BLOCK\" in last_user_message.upper():\n",
        "        print(\"[Callback] 'BLOCK' keyword found. Skipping LLM call.\")\n",
        "        # Return an LlmResponse to skip the actual LLM call\n",
        "        return LlmResponse(\n",
        "            content=types.Content(\n",
        "                role=\"model\",\n",
        "                parts=[\n",
        "                    types.Part(text=\"LLM call was blocked by before_model_callback.\")\n",
        "                ],\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"[Callback] Proceeding with LLM call.\")\n",
        "        # Return None to allow the (modified) request to go to the LLM\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"ModelCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a helpful assistant.\",  # Base instruction\n",
        "        description=\"An LLM agent demonstrating before_model_callback\",\n",
        "        before_model_callback=simple_before_model_modifier,  # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(\n",
        "        agent=my_llm_agent, app_name=\"llm_model_cb_app\", session_service=session_service\n",
        "    )\n",
        "    session_id_run = \"model_cb_run_1\"\n",
        "    session_id_block = \"model_cb_block_1\"\n",
        "    user_id = \"model_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_model_cb_app\", user_id=user_id, session_id=session_id_run\n",
        "    )\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_model_cb_app\", user_id=user_id, session_id=session_id_block\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"--- Running LLM Agent Normally (with before_model_callback modification) ---\"\n",
        "    )\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_run,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"Tell me a short joke.\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with BLOCK Keyword (triggering skip) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_block,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"BLOCK this request.\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_12_'></a>[LLM Agent with after_model_callback and state](#toc0_)\n",
        "\n",
        "This use case demonstract\n",
        "- overwriting model response with custom text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "import copy  # Needed to safely modify response content\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred model\n",
        "\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_model_modifier(\n",
        "    callback_context: CallbackContext, llm_response: LlmResponse\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Inspects/modifies the LLM response after it's received.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    print(f\"[Callback] After model call for agent: {agent_name}\")\n",
        "\n",
        "    # --- Inspection ---\n",
        "    original_text = \"\"\n",
        "    if llm_response.content and llm_response.content.parts:\n",
        "        # Assuming simple text response for this example\n",
        "        if llm_response.content.parts[0].text:\n",
        "            original_text = llm_response.content.parts[0].text\n",
        "            print(\n",
        "                f\"[Callback] Inspected original response text: '{original_text[:100]}...'\"\n",
        "            )  # Log snippet\n",
        "        elif llm_response.content.parts[0].function_call:\n",
        "            print(\n",
        "                f\"[Callback] Inspected response: Contains function call '{llm_response.content.parts[0].function_call.name}'. No text modification.\"\n",
        "            )\n",
        "            return None  # Don't modify tool calls in this example\n",
        "        else:\n",
        "            print(\"[Callback] Inspected response: No text content found.\")\n",
        "            return None\n",
        "    elif llm_response.error_message:\n",
        "        print(\n",
        "            f\"[Callback] Inspected response: Contains error '{llm_response.error_message}'. No modification.\"\n",
        "        )\n",
        "        return None\n",
        "    else:\n",
        "        print(\"[Callback] Inspected response: Empty LlmResponse.\")\n",
        "        return None  # Nothing to modify\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # Replace \"scientists\" with \"funny scientists\" (case-insensitive)\n",
        "    search_term = \"scientists\"\n",
        "    replace_term = \"CUSTOM scientists\"\n",
        "    if search_term in original_text.lower():\n",
        "        print(f\"[Callback] Found '{search_term}'. Modifying response.\")\n",
        "        modified_text = original_text.replace(search_term, replace_term)\n",
        "        modified_text = modified_text.replace(\n",
        "            search_term.capitalize(), replace_term.capitalize()\n",
        "        )  # Handle capitalization\n",
        "\n",
        "        # Create a NEW LlmResponse with the modified content\n",
        "        # Deep copy parts to avoid modifying original if other callbacks exist\n",
        "        modified_parts = [copy.deepcopy(part) for part in llm_response.content.parts]\n",
        "        modified_parts[0].text = modified_text  # Update the text in the copied part\n",
        "\n",
        "        new_response = LlmResponse(\n",
        "            content=types.Content(role=\"model\", parts=modified_parts),\n",
        "            # Copy other relevant fields if necessary, e.g., grounding_metadata\n",
        "            grounding_metadata=llm_response.grounding_metadata,\n",
        "        )\n",
        "        print(f\"[Callback] Returning modified response.\")\n",
        "        return new_response  # Return the modified response\n",
        "    else:\n",
        "        print(\n",
        "            f\"[Callback] '{search_term}' not found. Passing original response through.\"\n",
        "        )\n",
        "        # Return None to use the original llm_response\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"AfterModelCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a helpful assistant.\",\n",
        "        description=\"An LLM agent demonstrating after_model_callback\",\n",
        "        after_model_callback=simple_after_model_modifier,  # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(\n",
        "        agent=my_llm_agent,\n",
        "        app_name=\"llm_after_model_cb_app\",\n",
        "        session_service=session_service,\n",
        "    )\n",
        "    session_id_run = \"after_model_cb_run_1\"\n",
        "    session_id_modify = \"after_model_cb_modify_1\"\n",
        "    user_id = \"after_model_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_after_model_cb_app\", user_id=user_id, session_id=session_id_run\n",
        "    )\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_after_model_cb_app\", user_id=user_id, session_id=session_id_modify\n",
        "    )\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally (Callback passes response through) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_run,\n",
        "        new_message=types.Content(role=\"user\", parts=[types.Part(text=\"Say hello.\")]),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Input Triggering Modification ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_modify,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"Why don't scientists trust atoms\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_13_'></a>[LLM Agent with before_tool_callback and state](#toc0_)\n",
        "\n",
        "This cell demonstract:\n",
        "- inspection of tool arguments, and overwriting of argument and response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional, Dict, Any\n",
        "import copy\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "\n",
        "# Need LlmRequest, LlmResponse for context\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "\n",
        "# Need BaseTool, ToolContext for the callback signature\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "\n",
        "# Using FunctionTool to easily create a tool\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred model\n",
        "\n",
        "\n",
        "# --- Define a Simple Tool Function ---\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
        "    print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
        "    country_capitals = {\n",
        "        \"united states\": \"Washington, D.C.\",\n",
        "        \"canada\": \"Ottawa\",  # Intentionally correct here\n",
        "        \"france\": \"Paris\",\n",
        "        \"germany\": \"Berlin\",\n",
        "    }\n",
        "    return country_capitals.get(country.lower(), f\"Capital not found for {country}\")\n",
        "\n",
        "\n",
        "# --- Wrap the function into a Tool ---\n",
        "capital_tool = FunctionTool(func=get_capital_city)\n",
        "\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_before_tool_modifier(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Inspects/modifies tool args or skips the tool call.\"\"\"\n",
        "    agent_name = tool_context.agent_name\n",
        "    tool_name = tool.name\n",
        "    print(f\"[Callback] Before tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
        "    print(f\"[Callback] Original args: {args}\")\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # If the tool is 'get_capital_city' and country is 'Canada', change it to 'France'\n",
        "    if tool_name == \"get_capital_city\" and args.get(\"country\", \"\").lower() == \"canada\":\n",
        "        print(\"[Callback] Detected 'Canada'. Modifying args to 'France'.\")\n",
        "        args[\"country\"] = \"France\"  # Modify the args dictionary directly\n",
        "        print(f\"[Callback] Modified args: {args}\")\n",
        "        return None  # Proceed with modified args\n",
        "\n",
        "    # --- Skip Example ---\n",
        "    # If the tool is 'get_capital_city' and country is 'BLOCK'\n",
        "    if tool_name == \"get_capital_city\" and args.get(\"country\", \"\").upper() == \"BLOCK\":\n",
        "        print(\"[Callback] Detected 'BLOCK'. Skipping tool execution.\")\n",
        "        # Return a dictionary to be used as the tool result, skipping the actual tool call\n",
        "        return {\"result\": \"Tool execution was blocked by before_tool_callback.\"}\n",
        "\n",
        "    print(\"[Callback] Proceeding with original or previously modified args.\")\n",
        "    # Return None to allow the tool to execute normally (with original or modified args)\n",
        "    return None\n",
        "\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent with the tool and callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"ToolCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are an agent that can find capital cities. Use the get_capital_city tool.\",\n",
        "        description=\"An LLM agent demonstrating before_tool_callback\",\n",
        "        tools=[capital_tool],  # Add the tool here\n",
        "        before_tool_callback=simple_before_tool_modifier,  # Assign the callback here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(\n",
        "        agent=my_llm_agent, app_name=\"llm_tool_cb_app\", session_service=session_service\n",
        "    )\n",
        "    session_id_run = \"tool_cb_run_1\"\n",
        "    session_id_modify = \"tool_cb_modify_1\"\n",
        "    session_id_block = \"tool_cb_block_1\"\n",
        "    user_id = \"tool_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_run\n",
        "    )\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_modify\n",
        "    )\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_block\n",
        "    )\n",
        "\n",
        "    print(\"--- Running Agent (Normal Tool Call - Germany) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_run,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"What is the capital of Germany?\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        \"\\n--- Running Agent (Tool Call Triggering Modification - Canada -> France) ---\"\n",
        "    )\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_modify,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"What is the capital of Canada?\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "    print(\"\\n--- Running Agent (Tool Call Triggering Skip - BLOCK) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_block,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"What is the capital of BLOCK?\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_14_'></a>[LLM Agent with after_tool_callback and state](#toc0_)\n",
        "\n",
        "- Modify tool call result based on tool argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional, Dict, Any\n",
        "import copy  # Good practice for modifying results\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred model\n",
        "\n",
        "\n",
        "# --- Define a Simple Tool Function (Same as before) ---\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
        "    print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
        "    country_capitals = {\n",
        "        \"united states\": \"Washington, D.C.\",\n",
        "        \"canada\": \"Ottawa\",\n",
        "        \"france\": \"Paris\",\n",
        "        \"germany\": \"Berlin\",\n",
        "    }\n",
        "    return {\n",
        "        \"result\": country_capitals.get(\n",
        "            country.lower(), f\"Capital not found for {country}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Wrap the function into a Tool ---\n",
        "capital_tool = FunctionTool(func=get_capital_city)\n",
        "\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_tool_modifier(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Inspects/modifies the tool result after execution.\"\"\"\n",
        "    agent_name = tool_context.agent_name\n",
        "    tool_name = tool.name\n",
        "    print(f\"[Callback] After tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
        "    print(f\"[Callback] Args used: {args}\")\n",
        "    print(f\"[Callback] Original tool_response: {tool_response}\")\n",
        "\n",
        "    # Default structure for function tool results is {\"result\": <return_value>}\n",
        "    original_result_value = tool_response.get(\"result\", \"\")\n",
        "    # original_result_value = tool_response\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # If the tool was 'get_capital_city' and result is 'Washington, D.C.'\n",
        "    if tool_name == \"get_capital_city\" and original_result_value == \"Washington, D.C.\":\n",
        "        print(\"[Callback] Detected 'Washington, D.C.'. Modifying tool response.\")\n",
        "\n",
        "        # IMPORTANT: Create a new dictionary or modify a copy\n",
        "        modified_response = copy.deepcopy(tool_response)\n",
        "        modified_response[\"result\"] = (\n",
        "            f\"{original_result_value} (Note: This is the capital of the USA).\"\n",
        "        )\n",
        "        modified_response[\"note_added_by_callback\"] = True  # Add extra info if needed\n",
        "\n",
        "        print(f\"[Callback] Modified tool_response: {modified_response}\")\n",
        "        return modified_response  # Return the modified dictionary\n",
        "\n",
        "    print(\"[Callback] Passing original tool response through.\")\n",
        "    # Return None to use the original tool_response\n",
        "    return None\n",
        "\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent with the tool and callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"AfterToolCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are an agent that finds capital cities using the get_capital_city tool. Report the result clearly.\",\n",
        "        description=\"An LLM agent demonstrating after_tool_callback\",\n",
        "        tools=[capital_tool],  # Add the tool\n",
        "        after_tool_callback=simple_after_tool_modifier,  # Assign the callback\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(\n",
        "        agent=my_llm_agent,\n",
        "        app_name=\"llm_after_tool_cb_app\",\n",
        "        session_service=session_service,\n",
        "    )\n",
        "    session_id_run = \"after_tool_cb_run_1\"\n",
        "    session_id_modify = \"after_tool_cb_modify_1\"\n",
        "    user_id = \"after_tool_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_after_tool_cb_app\", user_id=user_id, session_id=session_id_run\n",
        "    )\n",
        "    session_service.create_session(\n",
        "        app_name=\"llm_after_tool_cb_app\", user_id=user_id, session_id=session_id_modify\n",
        "    )\n",
        "\n",
        "    print(\"--- Running Agent (Callback passes result through - France) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_run,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\", parts=[types.Part(text=\"What is the capital of France?\")]\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "    print(\"\\n--- Running Agent (Callback modifies result - United States) ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id_modify,\n",
        "        new_message=types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part(text=\"What is the capital of the United States?\")],\n",
        "        ),\n",
        "    ):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(\n",
        "                f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_15_'></a>[LLM Agent with Gaurdrail (Profanity Checker with before_model callback)](#toc0_)\n",
        "\n",
        "- Prevent model calling if there is bad word detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xexSYqxPuSiH"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents.llm_agent import AfterModelCallback, BeforeModelCallback\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from typing import Any, List, Optional\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "\n",
        "def profanity_guardrail(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Check for profanity in the model request.\"\"\"\n",
        "    profanity_list: List[str] = [\"badword1\", \"badword2\", \"badword3\"]\n",
        "    if llm_request.contents:\n",
        "        for content in llm_request.contents:\n",
        "            for part in content.parts:\n",
        "                if part.text:\n",
        "                    for profanity in profanity_list:\n",
        "                        if profanity in part.text.lower():\n",
        "                            callback_context.state[\"profanity_trigger\"] = True\n",
        "                            return LlmResponse(\n",
        "                                content=types.Content(\n",
        "                                    role=\"model\",\n",
        "                                    parts=[types.Part(text=(\"No bad word allowed.\"))],\n",
        "                                )\n",
        "                            )\n",
        "    return None\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        \"chicago\": {\"temperature\": 25, \"condition\": \"sunny\", \"sky\": \"clear\"},\n",
        "        \"toronto\": {\"temperature\": 30, \"condition\": \"partly cloudy\", \"sky\": \"overcast\"},\n",
        "        \"chennai\": {\"temperature\": 15, \"condition\": \"rainy\", \"sky\": \"cloudy\"},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "async def run_query(query: str):\n",
        "    weather_agent = LlmAgent(\n",
        "        model=GEMINI_2_FLASH,\n",
        "        name=\"weather_agent\",\n",
        "        instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "        When a user provides a prompt, extract the city name.\n",
        "        Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "        Finally, present the weather information to the user in a clear and concise manner.\n",
        "        If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "        description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "        You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "        tools=[get_weather],\n",
        "        before_model_callback=profanity_guardrail,\n",
        "    )\n",
        "\n",
        "    session_service = InMemorySessionService()\n",
        "    session = await session_service.create_session(\n",
        "        app_name=\"weather_app\", user_id=\"12345\", session_id=\"123344\"\n",
        "    )\n",
        "    runner = Runner(\n",
        "        agent=weather_agent,\n",
        "        app_name=\"weather_app\",\n",
        "        session_service=session_service,\n",
        "    )\n",
        "\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(\n",
        "        user_id=\"12345\", session_id=\"123344\", new_message=content\n",
        "    ):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Agent Response: {final_response}\")\n",
        "            print(\"-\" * 20)\n",
        "        if \"profanity_trigger\" in event.actions.state_delta:\n",
        "            print(\n",
        "                f\"Profanity Triggered: {event.actions.state_delta['profanity_trigger']}\"\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K2D004BzaEv",
        "outputId": "b2b789fb-166c-4392-9721-9b4ec7156e14"
      },
      "outputs": [],
      "source": [
        "await run_query(\"What is the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await run_query(\"what the badword1 is the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_16_'></a>[LlmAgent with All Callbacks use case](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Workflow for Query: 'Help! My computer screen keeps flickering constantly.' ---\n",
            "\n",
            "[User Submits Ticket: 'Help! My computer screen keeps flickering constantly.']\n",
            "\n",
            "[Callback Triggered: before_agent_callback]\n",
            "  -> Processing Ticket ID: TICKET-ABC\n",
            "  -> Agent 'support_agent' starting.\n",
            "\n",
            "[Callback Triggered: before_model_callback]\n",
            "  -> Preparing to call LLM for agent 'support_agent'.\n",
            "  -> Safety check/prompt augmentation applied (simulated).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Callback Triggered: after_model_callback]\n",
            "  -> Received LLM response for agent 'support_agent'.\n",
            "  -> LLM Raw Response (brief): Okay, I understand your computer screen is flickering. I can help you with that.\n",
            "\n",
            "...\n",
            "  -> PII check passed (simulated).\n",
            "\n",
            "[AFW: LLM decided to use Tool 'kb_search']\n",
            "\n",
            "[Callback Triggered: before_tool_callback for Tool: 'kb_search']\n",
            "  -> Attempting to call tool 'kb_search' with args: {'keywords': ['screen', 'flickering']}\n",
            "  -> Keyword validation passed.\n",
            "      [Tool Executing: kb_search with keywords: ['screen', 'flickering']]\n",
            "      [Tool Result: Found 4 steps]\n",
            "\n",
            "[Callback Triggered: after_tool_callback for Tool: 'kb_search']\n",
            "  -> Tool 'kb_search' executed with args: {'keywords': ['screen', 'flickering']}\n",
            "  -> Received tool response (brief): {'solutions': ['Check the display cable connection.', 'Try a different monitor port.', 'Update the graphics driver.', 'Adjust the screen refresh rate.']}...\n",
            "  -> Caching tool result (simulated).\n",
            "\n",
            "[AFW: Received result from Tool 'kb_search']\n",
            "  -> AFW: Sending tool result back to LLM for final response generation...\n",
            "\n",
            "[Callback Triggered: before_model_callback]\n",
            "  -> Preparing to call LLM for agent 'support_agent'.\n",
            "  -> Safety check/prompt augmentation applied (simulated).\n",
            "\n",
            "[Callback Triggered: after_model_callback]\n",
            "  -> Received LLM response for agent 'support_agent'.\n",
            "  -> LLM Raw Response (brief): Okay, I have some troubleshooting steps for you. Please try the following:\n",
            "\n",
            "1.  **Check the display ...\n",
            "  -> PII check passed (simulated).\n",
            "\n",
            "[AFW: Sending Final Response to User]\n",
            "  -> Response: Okay, I have some troubleshooting steps for you. Please try the following:\n",
            "\n",
            "1.  **Check the display cable connection:** Make sure the cable connecting your monitor to your computer is securely plugged in at both ends. Try disconnecting and reconnecting it.\n",
            "2.  **Try a different monitor port:** If you're using a desktop, try plugging the monitor into a different port on your computer (e.g., switch from HDMI to DisplayPort, or try a different HDMI port).\n",
            "3.  **Update the graphics driver:** Outdated or corrupted graphics drivers can cause screen flickering. You can usually update your drivers through the device manager or by downloading the latest drivers from the manufacturer's website (NVIDIA, AMD, Intel).\n",
            "4.  **Adjust the screen refresh rate:** Sometimes, an incorrect refresh rate can cause flickering. Go to your display settings and try a different refresh rate (usually 60Hz is a safe bet).\n",
            "\n",
            "\n",
            "[Callback Triggered: after_agent_callback]\n",
            "  -> Finished processing for Ticket ID: TICKET-ABC.\n",
            "  -> Updating external system: Ticket TICKET-ABC status set to 'Responded'.\n",
            "\n",
            "--- Workflow Finished for Query: 'Help! My computer screen keeps flickering constantly.' ---\n",
            "Final Session State (example): ticket_id='TICKET-ABC', status='completed'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Okay, I have some troubleshooting steps for you. Please try the following:\\n\\n1.  **Check the display cable connection:** Make sure the cable connecting your monitor to your computer is securely plugged in at both ends. Try disconnecting and reconnecting it.\\n2.  **Try a different monitor port:** If you're using a desktop, try plugging the monitor into a different port on your computer (e.g., switch from HDMI to DisplayPort, or try a different HDMI port).\\n3.  **Update the graphics driver:** Outdated or corrupted graphics drivers can cause screen flickering. You can usually update your drivers through the device manager or by downloading the latest drivers from the manufacturer's website (NVIDIA, AMD, Intel).\\n4.  **Adjust the screen refresh rate:** Sometimes, an incorrect refresh rate can cause flickering. Go to your display settings and try a different refresh rate (usually 60Hz is a safe bet).\\n\""
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import warnings\n",
        "import os\n",
        "from typing import Any, Optional, Dict, List, AsyncGenerator\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents import Agent, LlmAgent, BaseAgent  # Using LlmAgent directly\n",
        "from google.adk.sessions import InMemorySessionService, Session, State\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.events import Event\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.genai import types\n",
        "\n",
        "# Suppress specific UserWarning from google.generativeai if necessary\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=UserWarning, module=\"google.generativeai.types.content_types\"\n",
        ")\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"support_ticket_app\"\n",
        "USER_ID = \"customer_123\"\n",
        "SESSION_ID = \"ticket_session_abc\"\n",
        "AGENT_NAME = \"support_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"  # Or your preferred Gemini model\n",
        "\n",
        "\n",
        "# --- Simulated Knowledge Base Tool ---\n",
        "def kb_search(keywords: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Searches the knowledge base for troubleshooting steps based on keywords.\n",
        "\n",
        "    Args:\n",
        "        keywords: A list of keywords related to the issue (e.g., ['screen', 'flickering']).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing potential solutions or an empty dictionary if none found.\n",
        "    \"\"\"\n",
        "    print(f\"      [Tool Executing: kb_search with keywords: {keywords}]\")\n",
        "    # Simple mock implementation\n",
        "    mock_kb = {\n",
        "        \"screen\": [\n",
        "            \"Check the display cable connection.\",\n",
        "            \"Try a different monitor port.\",\n",
        "        ],\n",
        "        \"flickering\": [\n",
        "            \"Update the graphics driver.\",\n",
        "            \"Adjust the screen refresh rate.\",\n",
        "        ],\n",
        "        \"display\": [\"Ensure monitor power is on.\", \"Reboot the computer.\"],\n",
        "        \"keyboard\": [\"Check battery if wireless.\", \"Try a different USB port.\"],\n",
        "    }\n",
        "    results = []\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in mock_kb:\n",
        "            results.extend(mock_kb[keyword.lower()])\n",
        "\n",
        "    if results:\n",
        "        # Remove duplicates while preserving order (if Python 3.7+)\n",
        "        unique_results = list(dict.fromkeys(results))\n",
        "        print(f\"      [Tool Result: Found {len(unique_results)} steps]\")\n",
        "        return {\"solutions\": unique_results}\n",
        "    else:\n",
        "        print(\"      [Tool Result: No relevant KB articles found]\")\n",
        "        return {\n",
        "            \"solutions\": [\n",
        "                \"No specific troubleshooting steps found in KB for these keywords.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "# Wrap the function into a FunctionTool\n",
        "kb_search_tool = FunctionTool(func=kb_search)\n",
        "\n",
        "\n",
        "# --- Callback Implementations ---\n",
        "\n",
        "\n",
        "def log_before_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Callback executed before the agent starts processing.\"\"\"\n",
        "    ticket_id = (\n",
        "        f\"TICKET-{SESSION_ID.split('_')[-1].upper()}\"  # Simulate getting ticket ID\n",
        "    )\n",
        "    print(f\"\\n[Callback Triggered: before_agent_callback]\")\n",
        "    print(f\"  -> Processing Ticket ID: {ticket_id}\")\n",
        "    # Example state modification: Store ticket ID\n",
        "    callback_context.state[\"ticket_id\"] = ticket_id\n",
        "    print(f\"  -> Agent '{callback_context.agent_name}' starting.\")\n",
        "    return None  # Return None to allow agent execution\n",
        "\n",
        "\n",
        "def log_after_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Callback executed after the agent finishes processing.\"\"\"\n",
        "    ticket_id = callback_context.state.get(\"ticket_id\", \"UNKNOWN\")\n",
        "    print(f\"\\n[Callback Triggered: after_agent_callback]\")\n",
        "    print(f\"  -> Finished processing for Ticket ID: {ticket_id}.\")\n",
        "    # Example: Simulate updating ticket status in an external system\n",
        "    print(\n",
        "        f\"  -> Updating external system: Ticket {ticket_id} status set to 'Responded'.\"\n",
        "    )\n",
        "    # Example state modification\n",
        "    callback_context.state[\"processing_status\"] = \"completed\"\n",
        "    return None  # Return None, we don't want to append extra content here\n",
        "\n",
        "\n",
        "def log_before_model(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Callback executed before sending the request to the LLM.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: before_model_callback]\")\n",
        "    print(f\"  -> Preparing to call LLM for agent '{callback_context.agent_name}'.\")\n",
        "    # Example: Log request details (be careful with PII in real scenarios)\n",
        "    # print(f\"  -> LLM Request Contents (brief): {str(llm_request.contents)[:200]}...\")\n",
        "    # Example: Add a safety reminder (Note: modifying system_instruction directly might be overwritten)\n",
        "    # llm_request.append_instructions([\"Remember to be helpful and safe.\"]) # Use append_instructions\n",
        "    print(f\"  -> Safety check/prompt augmentation applied (simulated).\")\n",
        "    return None  # Return None to proceed with LLM call\n",
        "\n",
        "\n",
        "def check_after_model(\n",
        "    callback_context: CallbackContext, llm_response: LlmResponse\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Callback executed after receiving the response from the LLM.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: after_model_callback]\")\n",
        "    print(f\"  -> Received LLM response for agent '{callback_context.agent_name}'.\")\n",
        "    # Example: Log response details (be careful with PII)\n",
        "    response_text = (\n",
        "        llm_response.content.parts[0].text\n",
        "        if llm_response.content and llm_response.content.parts\n",
        "        else \"[No Text]\"\n",
        "    )\n",
        "    print(f\"  -> LLM Raw Response (brief): {response_text[:100]}...\")\n",
        "    # Example: Simulate PII check\n",
        "    if \"password\" in response_text.lower() or \"credit card\" in response_text.lower():\n",
        "        print(\"  -> !! PII potentially detected in LLM response (simulated) !!\")\n",
        "        # Could modify response here, e.g., return an error or redacted text\n",
        "        # return LlmResponse(content=types.Content(parts=[types.Part(text=\"[Response redacted due to potential PII]\")]))\n",
        "    else:\n",
        "        print(\"  -> PII check passed (simulated).\")\n",
        "    return (\n",
        "        None  # Return None to use the original (or potentially modified) LLM response\n",
        "    )\n",
        "\n",
        "\n",
        "def validate_before_tool(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Callback executed before a tool is called.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: before_tool_callback for Tool: '{tool.name}']\")\n",
        "    print(f\"  -> Attempting to call tool '{tool.name}' with args: {args}\")\n",
        "    # Example: Validate arguments\n",
        "    if tool.name == \"kb_search\" and \"keywords\" in args:\n",
        "        if not isinstance(args[\"keywords\"], list) or not args[\"keywords\"]:\n",
        "            print(\n",
        "                \"  -> !! Validation Failed: Keywords must be a non-empty list. Skipping tool call. !!\"\n",
        "            )\n",
        "            return {\n",
        "                \"error\": \"Invalid keywords provided for KB search.\"\n",
        "            }  # Return error to LLM\n",
        "        print(\"  -> Keyword validation passed.\")\n",
        "    return None  # Return None to proceed with actual tool execution\n",
        "\n",
        "\n",
        "def log_after_tool(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Callback executed after a tool has run.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: after_tool_callback for Tool: '{tool.name}']\")\n",
        "    print(f\"  -> Tool '{tool.name}' executed with args: {args}\")\n",
        "    print(f\"  -> Received tool response (brief): {str(tool_response)[:200]}...\")\n",
        "    # Example: Caching simulation (just log it)\n",
        "    print(f\"  -> Caching tool result (simulated).\")\n",
        "    # Example: Modify response if needed\n",
        "    # if \"solutions\" in tool_response and tool_response[\"solutions\"]:\n",
        "    #    tool_response[\"solutions\"].append(\"Also, try restarting your device.\") # Append suggestion\n",
        "    return None  # Return None to use the original (or modified) tool response\n",
        "\n",
        "\n",
        "# --- Agent Definition ---\n",
        "support_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are an IT Support Agent. Your goal is to help users troubleshoot technical issues.\n",
        "1. Analyze the user's problem description (support ticket).\n",
        "2. Identify keywords related to the issue.\n",
        "3. If the issue relates to common hardware problems like 'screen', 'display', 'flickering', 'keyboard', use the `kb_search` tool with the identified keywords to find troubleshooting steps.\n",
        "4. Based on your analysis and any results from the `kb_search` tool, provide a clear, step-by-step response to the user.\n",
        "5. If the `kb_search` tool doesn't return useful information, state that and provide general troubleshooting advice (e.g., restart, check connections).\n",
        "\"\"\",\n",
        "    description=\"First-level IT support agent that analyzes issues and uses a knowledge base.\",\n",
        "    tools=[kb_search_tool],\n",
        "    # --- Assign Callbacks ---\n",
        "    before_agent_callback=log_before_agent,\n",
        "    after_agent_callback=log_after_agent,\n",
        "    before_model_callback=log_before_model,\n",
        "    after_model_callback=check_after_model,\n",
        "    before_tool_callback=validate_before_tool,\n",
        "    after_tool_callback=log_after_tool,\n",
        ")\n",
        "\n",
        "# --- Session and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "# Ensure session is created before running\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=support_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "async def call_agent_and_show_flow(query):\n",
        "    print(f\"\\n--- Starting Workflow for Query: '{query}' ---\")\n",
        "    print(f\"\\n[User Submits Ticket: '{query}']\")\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    final_response_text = \"[Agent did not produce a final response text]\"\n",
        "\n",
        "    async for event in runner.run_async(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    ):\n",
        "        # Print event details to trace the flow\n",
        "        # print(f\"\\nDEBUG Event: Author={event.author}, Partial={event.partial}, Final={event.is_final_response()}, Content={str(event.content)[:100]}...\")\n",
        "\n",
        "        if event.get_function_calls():\n",
        "            print(\n",
        "                f\"\\n[AFW: LLM decided to use Tool '{event.get_function_calls()[0].name}']\"\n",
        "            )\n",
        "            # Before tool callback is triggered internally by the runner/flow\n",
        "\n",
        "        elif event.get_function_responses():\n",
        "            print(\n",
        "                f\"\\n[AFW: Received result from Tool '{event.get_function_responses()[0].name}']\"\n",
        "            )\n",
        "            # After tool callback is triggered internally by the runner/flow\n",
        "            print(\n",
        "                \"  -> AFW: Sending tool result back to LLM for final response generation...\"\n",
        "            )\n",
        "\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "            # Check if there's text before accessing parts[0]\n",
        "            if event.content.parts[0].text:\n",
        "                final_response_text = event.content.parts[0].text\n",
        "                print(f\"\\n[AFW: Sending Final Response to User]\")\n",
        "                print(f\"  -> Response: {final_response_text}\")\n",
        "            else:\n",
        "                print(\n",
        "                    f\"\\n[AFW: Final Response - Non-text content received: {event.content.parts}]\"\n",
        "                )\n",
        "                final_response_text = \"[Non-text final response]\"\n",
        "            # After agent callback will be triggered after this loop finishes internally\n",
        "\n",
        "    print(f\"\\n--- Workflow Finished for Query: '{query}' ---\")\n",
        "    # Retrieve final state to show callback modification\n",
        "    final_session = await session_service.get_session(\n",
        "        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        "    )\n",
        "    print(\n",
        "        f\"Final Session State (example): ticket_id='{final_session.state.get('ticket_id')}', status='{final_session.state.get('processing_status')}'\"\n",
        "    )\n",
        "    return final_response_text\n",
        "\n",
        "\n",
        "await call_agent_and_show_flow(\"Help! My computer screen keeps flickering constantly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_17_'></a>[Session Service](#toc0_)\n",
        "\n",
        "- How to retreive session state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Session State: {}\n",
            "\n",
            "Agent Response:  Paris\n",
            "\n",
            "Final Session State: {'capital_city': 'Paris\\n'}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"capital_finder_app\"\n",
        "USER_ID = \"quickstart_user\"\n",
        "SESSION_ID = \"session_abc\"\n",
        "MODEL = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = LlmAgent(\n",
        "    model=MODEL,\n",
        "    name=\"CapitalFinderAgent\",\n",
        "    instruction=\"\"\"You are an agent that finds the capital of a given country.\n",
        "    When asked for the capital, respond *only* with the name of the capital city.\n",
        "    \"\"\",\n",
        "    output_key=\"capital_city\",  # Save the agent's final response text to state['capital_city']\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"\\nAgent Response: \", final_response)\n",
        "\n",
        "\n",
        "initial_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "print(f\"Initial Session State: {initial_session.state}\")  # Should be empty {}\n",
        "\n",
        "call_agent(\"What is the capital of france?\")\n",
        "\n",
        "final_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "print(\n",
        "    f\"Final Session State: {final_session.state}\"\n",
        ")  # Should now contain {'capital_city': 'Paris'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_18_'></a>[Session State - State Manupilation](#toc0_)\n",
        "\n",
        "- How to manually modify session state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jtLbWd9-cqih"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.events import Event, EventActions\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"task_manager_app\"\n",
        "USER_ID = \"test_user\"\n",
        "AGENT_NAME = \"task_manager_agent\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\"  # Or any suitable model\n",
        "\n",
        "# --- Agent Definition ---\n",
        "#  Simplified instruction, as we're handling logic directly\n",
        "task_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Task Management Agent. Respond to user requests to manage tasks.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "\n",
        "async def add_task(session, task_description):\n",
        "    \"\"\"Adds a task to the task list.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])  # Get current tasks (or empty list)\n",
        "    new_task_id = len(tasks) + 1\n",
        "    new_task = {\"id\": new_task_id, \"description\": task_description, \"status\": \"pending\"}\n",
        "    tasks.append(new_task)\n",
        "    # Use EventActions to update the state (delta update)\n",
        "    add_event = Event(\n",
        "        author=\"agent\", actions=EventActions(state_delta={\"user:tasks\": tasks})\n",
        "    )\n",
        "    await session_service.append_event(session, add_event)\n",
        "    return f\"Task '{task_description}' added with ID {new_task_id}.\"\n",
        "\n",
        "\n",
        "async def modify_task(session, task_id, new_status):\n",
        "    \"\"\"Modifies the status of a task.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    try:\n",
        "        task_id = int(task_id)  # Ensure task_id is an integer\n",
        "    except ValueError:\n",
        "        return \"Invalid task ID. Please provide a number.\"\n",
        "\n",
        "    for i, task in enumerate(tasks):\n",
        "        if task[\"id\"] == task_id:\n",
        "            tasks[i][\"status\"] = new_status\n",
        "            # Update state via EventActions\n",
        "            modify_event = Event(\n",
        "                author=\"agent\", actions=EventActions(state_delta={\"user:tasks\": tasks})\n",
        "            )\n",
        "            await session_service.append_event(session, modify_event)\n",
        "            return f\"Task {task_id} status updated to '{new_status}'.\"\n",
        "    return f\"Task with ID {task_id} not found.\"\n",
        "\n",
        "\n",
        "async def delete_task(session, task_id):\n",
        "    \"\"\"Deletes a task from the task list.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    try:\n",
        "        task_id = int(task_id)\n",
        "    except ValueError:\n",
        "        return \"Invalid task ID.  Please provide a number.\"\n",
        "\n",
        "    updated_tasks = [task for task in tasks if task[\"id\"] != task_id]\n",
        "    if len(updated_tasks) < len(tasks):\n",
        "        # Update state via EventActions\n",
        "        delete_event = Event(\n",
        "            author=\"agent\",\n",
        "            actions=EventActions(state_delta={\"user:tasks\": updated_tasks}),\n",
        "        )\n",
        "        await session_service.append_event(session, delete_event)\n",
        "        return f\"Task {task_id} deleted.\"\n",
        "    return f\"Task with ID {task_id} not found.\"\n",
        "\n",
        "\n",
        "def list_tasks(session):\n",
        "    \"\"\"Lists all tasks for the user.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    if not tasks:\n",
        "        return \"You have no tasks.\"\n",
        "    task_list_str = \"\\n\".join(\n",
        "        f\"{task['id']}: {task['description']} ({task['status']})\" for task in tasks\n",
        "    )\n",
        "    return f\"Your tasks:\\n{task_list_str}\"\n",
        "\n",
        "\n",
        "def call_agent(user_input, session):\n",
        "    \"\"\"Sends user input to the agent and processes events.\"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_input)])\n",
        "    events = runner.run_async(\n",
        "        user_id=USER_ID, session_id=session.id, new_message=content\n",
        "    )  # session.id, not SESSION_ID\n",
        "\n",
        "    final_response_text = \"\"\n",
        "    for event in events:\n",
        "        if event.content and event.content.role == \"model\":\n",
        "            final_response_text = event.content.parts[0].text\n",
        "            break  # Exit loop after getting the final response.\n",
        "\n",
        "    return final_response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgLYUT1Ycqb_",
        "outputId": "0ac9b658-9f92-48d5-86ba-4c9b1b0906b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created session with ID: f3378003-be0a-43af-8fa2-680cd24e9032\n"
          ]
        }
      ],
      "source": [
        "# 1. Create a Session (with initial state, if any)\n",
        "#  demonstrates: create_session, InMemorySessionService, initial state\n",
        "USER_ID = \"test_user2\"\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID\n",
        ")  #  Let the service generate the ID\n",
        "\n",
        "print(f\"Created session with ID: {session.id}\")\n",
        "runner = Runner(agent=task_agent, app_name=APP_NAME, session_service=session_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G8I0MtT4Gfy",
        "outputId": "b2dba833-1a26-423a-cbf7-27ef7cecfbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nRetrieved session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tGg1H36a2k-8",
        "outputId": "8e47f9f9-2882-4e0e-ee5b-b3e625a9f25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Adding tasks directly to state via Function...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Task 'Buy groceries' added with ID 4.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nAdding tasks directly to state via Function...\")\n",
        "\n",
        "await add_task(session, \"Buy milk\")\n",
        "await add_task(session, \"Walk the dog\")\n",
        "await add_task(session, \"Prepare presentation\")\n",
        "await add_task(session, \"Buy groceries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSK4sovb1OWx",
        "outputId": "3ef0996b-e931-4758-e456-1d0afdf2ee37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'capital_city': 'Paris\\n',\n",
              " 'user:tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'},\n",
              "  {'id': 2, 'description': 'Walk the dog', 'status': 'pending'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Retrieve and display the session (demonstrates get_session)\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nRetrieved session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUOEucZl1VdT",
        "outputId": "24dd38a5-7f5a-4dcc-c238-917d19f45a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Listing tasks...\n",
            "Your tasks:\n",
            "1: Buy milk (pending)\n",
            "2: Walk the dog (pending)\n",
            "3: Prepare presentation (pending)\n",
            "4: Buy groceries (pending)\n"
          ]
        }
      ],
      "source": [
        "# 4. List tasks (demonstrates accessing state)\n",
        "print(\"\\nListing tasks...\")\n",
        "print(list_tasks(retrieved_session))  # Using a helper, accessing state directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3EWU1Tb14Vn",
        "outputId": "f945c5dd-c3c7-4b64-9752-136ff6a74b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Modifying task 2...\n",
            "Task 1 status updated to 'pending'.\n",
            "Task 2 status updated to 'completed'.\n",
            "Modified session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'capital_city': 'Paris\\n',\n",
              " 'user:tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'},\n",
              "  {'id': 2, 'description': 'Walk the dog', 'status': 'completed'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Modify a task (demonstrates state modification)\n",
        "print(\"\\nModifying task 2...\")\n",
        "print(await modify_task(retrieved_session, \"1\", \"pending\"))\n",
        "print(await modify_task(retrieved_session, \"2\", \"completed\"))\n",
        "\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"Modified session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yHQmUMB16c4",
        "outputId": "ab0f2fd1-1d4a-4831-84b6-cdc1fac331cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deleting task 1...\n",
            "Task 1 deleted.\n"
          ]
        }
      ],
      "source": [
        "# 6. Delete a task (demonstrates state deletion)\n",
        "print(\"\\nDeleting task 1...\")\n",
        "print(await delete_task(retrieved_session, \"1\"))\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFHKk3zA18nz",
        "outputId": "3d287272-a29f-4b79-a465-3661acf1845f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session state after deletion:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'capital_city': 'Paris\\n',\n",
              " 'user:tasks': [{'id': 2,\n",
              "   'description': 'Walk the dog',\n",
              "   'status': 'completed'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Session state after deletion:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTsLpN4a2Ddr",
        "outputId": "481bcb3b-d3cd-4580-c90d-2b15d8998f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sessions for user quickstart_user:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ListSessionsResponse(sessions=[Session(id='session_abc', app_name='capital_finder_app', user_id='quickstart_user', state={}, events=[], last_update_time=1748390047.337884)])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 7. List sessions for the user (demonstrates list_sessions)\n",
        "# Demonstrates:  list_sessions\n",
        "print(f\"\\nSessions for user {USER_ID}:\")\n",
        "await session_service.list_sessions(app_name=APP_NAME, user_id=USER_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjZI2ZgK2GDc",
        "outputId": "6a2ded3d-6d11-49eb-8ad5-39783b63c9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deleted session: session_abc\n"
          ]
        }
      ],
      "source": [
        "# 8. Delete a session (demonstrates delete_session).\n",
        "await session_service.delete_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nDeleted session: {session.id}\")\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session deleted successfully, as could not be retrieved.\n"
          ]
        }
      ],
      "source": [
        "if retrieved_session:  # Should be None.\n",
        "    print(retrieved_session)\n",
        "else:\n",
        "    print(\"Session deleted successfully, as could not be retrieved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_19_'></a>[Session State - delta_states](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Demonstration the difference between application, user type session state lifecycle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'notes': 'Initial order', 'tracking_number': 'XYZ456'}\n",
            "\n",
            "State with prefixes added:\n",
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'notes': 'Initial order', 'tracking_number': 'XYZ456', 'temp:request_id': 'temp_value'}\n",
            "\n",
            "State after retrieval (temp should be gone):\n",
            "{'order_status': 'pending', 'items': ['shirt'], 'notes': 'Initial order', 'app:name': 'app_weiyih', 'user:name': 'user_weiyih'}\n",
            "\n",
            "App State:\n",
            "{'my_app': {'name': 'app_weiyih'}, 'test': {'max_retries': 3}}\n",
            "\n",
            "User State:\n",
            "{'my_app': {'user1': {'name': 'user_weiyih'}}, 'test': {'user1': {'pref_contact': 'email'}}}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "\n",
        "# Create the session service\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Create a session (with initial state)\n",
        "session = await session_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"user1\",\n",
        "    state={\"order_status\": \"pending\", \"items\": [\"shirt\"], \"notes\": \"Initial order\"},\n",
        ")\n",
        "\n",
        "# --- Direct State Manipulation (Generally NOT Recommended) ---\n",
        "\n",
        "# 1. Retrieve the session (to get the current state)\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "\n",
        "# 2. Access the state dictionary directly\n",
        "current_state = retrieved_session.state\n",
        "\n",
        "# --- Simulating a Delta Update (Directly) ---\n",
        "\n",
        "# state specific to the session id\n",
        "current_state[\"order_status\"] = \"shipped\"  # Update existing key\n",
        "current_state[\"tracking_number\"] = \"XYZ456\"  # Add a new key\n",
        "current_state[\"items\"].append(\"pants\")  # Modify a list (append)\n",
        "\n",
        "state_changes = {\n",
        "    \"app:name\": \"app_weiyih\",\n",
        "    \"user:name\": \"user_weiyih\"\n",
        "}\n",
        "\n",
        "# --- Create Event with Actions ---\n",
        "actions_with_update = EventActions(state_delta=state_changes)\n",
        "# This event might represent an internal system action, not just an agent response\n",
        "system_event = Event(\n",
        "    invocation_id=\"123\",\n",
        "    author=\"system\", # Or 'agent', 'tool' etc.\n",
        "    actions=actions_with_update,\n",
        ")\n",
        "await session_service.append_event(session, system_event)\n",
        "\n",
        "# The changes are reflected *immediately* in the retrieved_session (with InMemorySessionService)\n",
        "# But update wont be persisted to the session service\n",
        "print(retrieved_session.state)\n",
        "\n",
        "# --- Demonstrating State Prefixes (Directly) ---\n",
        "# We need to use setdefault to properly initialize nested dictionaries.\n",
        "\n",
        "# state relevant to the entire application\n",
        "session_service.app_state.setdefault(\"test\", {})[\"max_retries\"] = 3\n",
        "#  state relevant to the specific user across sessions\n",
        "session_service.user_state.setdefault(\"test\", {}).setdefault(\"user1\", {})[\n",
        "    \"pref_contact\"\n",
        "] = \"email\"\n",
        "\n",
        "# temp prefix is specific to the current session processing turn.\n",
        "current_state[\"temp:request_id\"] = \"temp_value\"\n",
        "\n",
        "print(\"\\nState with prefixes added:\")\n",
        "print(retrieved_session.state)\n",
        "\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "print(\"\\nState after retrieval (temp should be gone):\")\n",
        "\n",
        "print(retrieved_session.state)\n",
        "\n",
        "print(\"\\nApp State:\")\n",
        "print(session_service.app_state)\n",
        "print(\"\\nUser State:\")\n",
        "print(session_service.user_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_20_'></a>[Accessing Session Properties](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "\n",
        "# Create a simple session to examine its properties\n",
        "temp_service = InMemorySessionService()\n",
        "example_session = temp_service.create_session(\n",
        "    app_name=\"my_app\", user_id=\"example_user\", state={\"initial_value\": 1}\n",
        ")\n",
        "\n",
        "print(f\"--- Examining Session Properties ---\")\n",
        "print(f\"ID (`id`):                {example_session.id}\")  # Unique identifier\n",
        "print(\n",
        "    f\"Application Name (`app_name`): {example_session.app_name}\"\n",
        ")  # Which app it belongs to\n",
        "print(f\"User ID (`user_id`):         {example_session.user_id}\")  # Who the user is\n",
        "print(\n",
        "    f\"State (`state`):           {example_session.state}\"\n",
        ")  # The dynamic 'notes' dictionary\n",
        "print(\n",
        "    f\"Events (`events`):         {example_session.events}\"\n",
        ")  # The conversation history (initially empty)\n",
        "print(\n",
        "    f\"Last Update (`last_update_time`): {example_session.last_update_time:.2f}\"\n",
        ")  # When it was last triggered\n",
        "print(f\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_21_'></a>[InMemory Session Service](#toc0_)\n",
        "\n",
        "- Adding event\n",
        "- Update state with `state_delta`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using InMemorySessionService Methods\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "from google.genai import types\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "print(\"\\n--- Demonstrating InMemorySessionService ---\")\n",
        "\n",
        "# 1. Instantiate\n",
        "session_service = InMemorySessionService()\n",
        "app_name, user_id = \"memory_app\", \"user_mem\"\n",
        "session_id = \"mem_session_1\"\n",
        "\n",
        "# 2. Create Session\n",
        "current_session = await session_service.create_session(\n",
        "    app_name=app_name, user_id=user_id, session_id=session_id, state={\"counter\": 0}\n",
        ")\n",
        "print(f\"Created Session: ID={current_session.id}, State={current_session.state}\")\n",
        "\n",
        "# 3. Append Event with State Delta\n",
        "user_event = Event(\n",
        "    invocation_id=\"inv_1\",\n",
        "    author=\"user\",\n",
        "    content=types.Content(parts=[types.Part(text=\"Increment\")]),\n",
        ")\n",
        "session_service.append_event(current_session, user_event)  # No state change yet\n",
        "\n",
        "agent_event = Event(\n",
        "    invocation_id=\"inv_2\",\n",
        "    author=\"agent\",\n",
        "    actions=EventActions(state_delta={\"counter\": 1}),  # Increment counter\n",
        ")\n",
        "session_service.append_event(current_session, agent_event)\n",
        "print(f\"Appended Event, state['counter'] should be 1\")\n",
        "\n",
        "# 4. Get Session\n",
        "retrieved_session = await session_service.get_session(\n",
        "    app_name=app_name, user_id=user_id, session_id=session_id\n",
        ")\n",
        "print(f\"Retrieved Session: ID={retrieved_session.id}, State={retrieved_session.state}\")\n",
        "print(\n",
        "    f\"Events in session: {len(retrieved_session.events)}\"\n",
        ")  # Shows 2 events were added\n",
        "\n",
        "# 5. List Sessions\n",
        "session_list = session_service.list_sessions(app_name=app_name, user_id=user_id)\n",
        "print(f\"List Sessions for {user_id}: {session_list}\")\n",
        "\n",
        "# 6. Delete Session\n",
        "session_service.delete_session(\n",
        "    app_name=app_name, user_id=user_id, session_id=session_id\n",
        ")\n",
        "print(f\"Deleted Session: {session_id}\")\n",
        "\n",
        "# 7. Get Session (should fail)\n",
        "deleted_session = await session_service.get_session(\n",
        "    app_name=app_name, user_id=user_id, session_id=session_id\n",
        ")\n",
        "print(\n",
        "    f\"Retrieve after delete: {'Session found (unexpected!)' if deleted_session else 'Session not found (correct)'}\"\n",
        ")\n",
        "print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_22_'></a>[Database Session Service (with SQLite for demo)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using DatabaseSessionService Methods (with SQLite for demo)\n",
        "\n",
        "# NOTE: Requires `sqlalchemy` to be installed.\n",
        "# NOTE: This creates a file 'db_sessions_demo.db' in the current directory.\n",
        "from google.adk.sessions import DatabaseSessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "from google.genai import types  # Make sure types is imported\n",
        "import time\n",
        "import uuid\n",
        "import os  # To manage the demo database file\n",
        "\n",
        "print(\"\\n--- Demonstrating DatabaseSessionService (SQLite) ---\")\n",
        "DB_FILE = \"./db_sessions_demo.db\"  # Define path for the database file\n",
        "DB_DIR = os.path.dirname(DB_FILE)  # Get directory path\n",
        "\n",
        "# Ensure the directory exists (useful if DB_FILE includes subdirectories)\n",
        "if DB_DIR and not os.path.exists(DB_DIR):\n",
        "    os.makedirs(DB_DIR)\n",
        "    print(f\"Created directory: {DB_DIR}\")\n",
        "\n",
        "# Remove the database file if it exists from a previous run for a clean demo\n",
        "if os.path.exists(DB_FILE):\n",
        "    os.remove(DB_FILE)\n",
        "    print(f\"Removed existing demo DB file: {DB_FILE}\")\n",
        "\n",
        "# 1. Instantiate (using SQLite file)\n",
        "# The DatabaseSessionService's __init__ method will handle DB/table creation.\n",
        "db_service = DatabaseSessionService(db_url=f\"sqlite:///{DB_FILE}\")\n",
        "print(f\"Instantiated DatabaseSessionService. DB file '{DB_FILE}' ensured/created.\")\n",
        "\n",
        "APP_DB, USER_DB = \"db_app\", \"user_db\"\n",
        "SESSION_ID_DB = \"db_session_1\"\n",
        "\n",
        "# 2. Create Session\n",
        "session_db = db_service.create_session(\n",
        "    app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB, state={\"status\": \"new\"}\n",
        ")\n",
        "print(f\"Created Session: ID={session_db.id}, State={session_db.state}\")\n",
        "\n",
        "# 3. Append Event with State Delta\n",
        "# *** FIX: Ensure event has a 'content' object, even if minimal ***\n",
        "event_db_1 = Event(\n",
        "    invocation_id=\"inv_db1\",\n",
        "    author=\"agent\",\n",
        "    content=types.Content(\n",
        "        parts=[types.Part(text=\"System update: Processing\")]\n",
        "    ),  # Add content\n",
        "    actions=EventActions(state_delta={\"status\": \"processing\", \"db_key\": \"db_val\"}),\n",
        ")\n",
        "# Note: append_event updates the state in the DB and the passed session's last_update_time\n",
        "db_service.append_event(session_db, event_db_1)\n",
        "print(f\"Appended Event, state should be updated in the database.\")\n",
        "\n",
        "# 4. Get Session (re-fetch from DB to see persisted changes)\n",
        "# Note: Must re-fetch session to see DB changes reflected in the object state\n",
        "retrieved_session_db = db_service.get_session(\n",
        "    app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB\n",
        ")\n",
        "print(\n",
        "    f\"Retrieved Session: ID={retrieved_session_db.id}, State={retrieved_session_db.state}\"\n",
        ")\n",
        "# Note: Events are not automatically loaded by get_session in this implementation by default.\n",
        "\n",
        "# 5. List Sessions\n",
        "sessions_list_db = db_service.list_sessions(app_name=APP_DB, user_id=USER_DB)\n",
        "print(f\"List Sessions for {USER_DB}: {sessions_list_db}\")\n",
        "\n",
        "# 6. List Events (Not Implemented in DatabaseSessionService base implementation)\n",
        "try:\n",
        "    db_service.list_events(app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB)\n",
        "except NotImplementedError as e:\n",
        "    print(f\"List Events: Received NotImplementedError (as expected in base class)\")\n",
        "except AttributeError as e:\n",
        "    print(\n",
        "        f\"List Events: Method not found or not implemented (as expected in base class)\"\n",
        "    )\n",
        "\n",
        "# 7. Delete Session\n",
        "db_service.delete_session(app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB)\n",
        "print(f\"Deleted Session: {SESSION_ID_DB}\")\n",
        "\n",
        "# 8. Get Session (should fail)\n",
        "deleted_session_check_db = db_service.get_session(\n",
        "    app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB\n",
        ")\n",
        "print(\n",
        "    f\"Retrieve after delete: {'Session found (unexpected!)' if deleted_session_check_db else 'Session not found (correct)'}\"\n",
        ")\n",
        "\n",
        "# Cleanup demo file (optional, good practice for demos)\n",
        "# if os.path.exists(DB_FILE):\n",
        "#     os.remove(DB_FILE)\n",
        "#     print(f\"Cleaned up demo DB file: {DB_FILE}\")\n",
        "print(\"--------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_23_'></a>[LlmAgent with Anthropic (3rd Party Model)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Anthropic Agent Example ---\n",
            "Using model: claude-3-7-sonnet@20250219\n",
            "Make sure GCP Project 'hello-world-418507' and Location 'us-east5' are correct and the model is available there.\n",
            "Ensure you are authenticated with GCP (e.g., `gcloud auth application-default login`).\n",
            "\n",
            "User Query: What is the capital of France?\n",
            "Agent Response: Paris\n",
            "\n",
            "User Query: What's the capital of Japan?\n",
            "Agent Response: Tokyo\n",
            "\n",
            "User Query: Tell me the capital of Germany.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import warnings\n",
        "\n",
        "# --- GCP/Vertex AI Configuration ---\n",
        "# Make sure these are set in your environment OR uncomment and set here\n",
        "# os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-project-id\"\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = (\n",
        "    \"us-east5\"  # Make sure its us-east5 or europe-west1\n",
        ")\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents import Agent, LlmAgent  # Using LlmAgent directly\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "from google.adk.models.anthropic_llm import Claude\n",
        "from google.adk.models.registry import LLMRegistry\n",
        "\n",
        "# Manually register the Claude model class with the registry\n",
        "# This step is crucial if the framework doesn't do it automatically\n",
        "LLMRegistry.register(Claude)\n",
        "\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"anthropic_capital_app\"\n",
        "USER_ID = \"anthropic_user\"\n",
        "SESSION_ID = \"anthropic_session_1\"\n",
        "AGENT_NAME = \"claude_capital_agent\"\n",
        "\n",
        "# --- Anthropic Model Name (via Vertex AI) ---\n",
        "# https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/sonnet-3-7\n",
        "\n",
        "ANTHROPIC_MODEL = \"claude-3-7-sonnet@20250219\"\n",
        "\n",
        "# --- Agent Definition ---\n",
        "# Simplest agent: takes user input, uses the LLM directly to answer.\n",
        "capital_agent = LlmAgent(\n",
        "    model=ANTHROPIC_MODEL,  # Specify the Anthropic model identifier\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"You are a helpful assistant. When asked for the capital of a country, provide only the name of the capital city.\",\n",
        "    description=\"An agent that provides the capital city of a country using an Anthropic model.\",\n",
        "    # No tools needed for this simple task\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "# --- Session and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "# Ensure session is created before running\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "# Using async version as it's preferred\n",
        "async def call_agent_async(query):\n",
        "    print(f\"\\nUser Query: {query}\")\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    final_response_text = \"Agent did not produce a final response.\"\n",
        "    try:\n",
        "        async for event in runner.run_async(\n",
        "            user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "        ):\n",
        "            if event.is_final_response() and event.content and event.content.parts:\n",
        "                final_response_text = event.content.parts[0].text\n",
        "                print(f\"Agent Response: {final_response_text}\")\n",
        "                # Break after final response for simplicity in this example\n",
        "                break\n",
        "            elif event.error_message:\n",
        "                final_response_text = f\"Agent Error: {event.error_message}\"\n",
        "                print(final_response_text)\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during agent execution: {e}\")\n",
        "        final_response_text = f\"Execution Error: {e}\"\n",
        "\n",
        "    return final_response_text\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "async def run_example():\n",
        "    print(\"--- Running Anthropic Agent Example ---\")\n",
        "    print(f\"Using model: {ANTHROPIC_MODEL}\")\n",
        "    print(\n",
        "        f\"Make sure GCP Project '{os.environ['GOOGLE_CLOUD_PROJECT']}' and Location '{os.environ['GOOGLE_CLOUD_LOCATION']}' are correct and the model is available there.\"\n",
        "    )\n",
        "    print(\n",
        "        \"Ensure you are authenticated with GCP (e.g., `gcloud auth application-default login`).\"\n",
        "    )\n",
        "\n",
        "    await call_agent_async(\"What is the capital of France?\")\n",
        "    await call_agent_async(\"What's the capital of Japan?\")\n",
        "    print(\"--- Example Finished ---\")\n",
        "\n",
        "\n",
        "await run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_24_'></a>[Artifact Service](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.adk.runners import Runner\n",
        "from google.adk.artifacts import InMemoryArtifactService\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.tools import ToolContext\n",
        "\n",
        "async def save_dummy_report(tool_context: ToolContext):\n",
        "    \"\"\"\n",
        "    Saves a dummy PDF report as an artifact.\n",
        "    \"\"\"\n",
        "\n",
        "    report_artifact = types.Part.from_bytes(\n",
        "        data=b\"this is dummy content\", mime_type=\"application/pdf\"\n",
        "    )\n",
        "    filename = \"generated_report.pdf\"\n",
        "    print(\"Generating dummy pdf\")\n",
        "    version = await tool_context.save_artifact(filename=filename, artifact=report_artifact)\n",
        "    return {\"status\": \"ok\", \"filename\": filename}\n",
        "\n",
        "async def load_dummy_report(tool_context: ToolContext)->str:\n",
        "    \"\"\"\n",
        "    Loads the dummy PDF report artifact from storage.\n",
        "    \"\"\"\n",
        "    filename = \"generated_report.pdf\"\n",
        "    report_artifact = await tool_context.load_artifact(filename=filename)\n",
        "\n",
        "    if report_artifact and report_artifact.inline_data:\n",
        "        print(f\"MIME Type: {report_artifact.inline_data.mime_type}\")\n",
        "        pdf_bytes = report_artifact.inline_data.data\n",
        "        print(f\"Report size: {len(pdf_bytes)} bytes.\")\n",
        "        return {\"status\": \"ok\", \"filename\": filename}\n",
        "    else:\n",
        "        return {\"status\": \"ok\", \"filename\":\"No artifact found\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating dummy pdf\n",
            "MIME Type: application/pdf\n",
            "Report size: 21 bytes.\n",
            "Agent Response:  I have saved a dummy report with filename generated_report.pdf, and I have also loaded it successfully.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "import uuid\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "session_service = InMemorySessionService()\n",
        "artifact_service = InMemoryArtifactService()\n",
        "\n",
        "# Required. Unique identifier for the application.\n",
        "APP_NAME = \"weather_app\"\n",
        "# Required. Identifier for the user interacting with the agent. This is a dynamic variable.\n",
        "USER_ID = \"12345\"\n",
        "\n",
        "SESSION_ID = f\"session_{uuid.uuid4()}\"  # Use a dynamic session ID\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "\n",
        "# Your agent definition\n",
        "root_agent = LlmAgent(\n",
        "    name=\"my_agent\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    tools=[save_dummy_report, load_dummy_report],\n",
        "    instruction=\"You are a helpful assistant that saves a dummy PDF report as an artifact, and then read it back to user.\",\n",
        ")\n",
        "\n",
        "runner = Runner(\n",
        "    agent=root_agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service,\n",
        "    artifact_service=artifact_service,\n",
        ")\n",
        "\n",
        "\n",
        "def call_agent(user_query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "# call_agent(\"load and retrieve the dummy report\")\n",
        "call_agent(\"call the tool save_dummy_report, and load_dummy_report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'weather_app/12345/session_444d04fc-04da-4725-8d6e-96e51a71b253/generated_report.pdf': [Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=Blob(data=b'this is dummy content', mime_type='application/pdf'), text=None)]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "artifact_service.artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_24_1_'></a>[Workflow Agents](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_25_'></a>[Sequence Agent](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import (\n",
        "    InvocationContext,\n",
        "    new_invocation_context_id,\n",
        ")\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Starting...\")]),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Starting...\")]),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = await session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(\n",
        "        name=\"SequentialAgent\", sub_agents=[agent_a, agent_b]\n",
        "    )\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(parts=[types.Part(text=\"execute\")]),\n",
        "    )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_25_1_'></a>[Passing state between Children](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Received value: Hello from Agent A!\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import (\n",
        "    InvocationContext,\n",
        "    new_invocation_context_id,\n",
        ")\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Starting...\")]),\n",
        "        )\n",
        "        # Set a value in the session state\n",
        "        ctx.session.state[\"agent_a_value\"] = \"Hello from Agent A!\"\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Starting...\")]),\n",
        "        )\n",
        "        # Retrieve the value from the session state\n",
        "        agent_a_value = ctx.session.state.get(\"agent_a_value\")\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=f\"Agent B: Received value: {agent_a_value}\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = await session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(\n",
        "        name=\"SequentialAgent\", sub_agents=[agent_a, agent_b]\n",
        "    )\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(parts=[types.Part(text=\"execute\")]),\n",
        "    )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_25_2_'></a>[Simple runner](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Agent A: Starting...\n",
            "\n",
            "Agent Response:  Agent B: Starting...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "\n",
        "APP_NAME = \"sequential_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"sequential_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "agent_a = LlmAgent(\n",
        "    name=\"AgentA\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    instruction=\"You are Agent A. Respond with 'Agent A: Starting...'\",\n",
        "    output_key=\"agent_a\",\n",
        ")\n",
        "\n",
        "agent_b = LlmAgent(\n",
        "    name=\"AgentB\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    instruction=\"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        ")\n",
        "\n",
        "# Create the SequentialAgent\n",
        "sequential_agent = SequentialAgent(\n",
        "    name=\"SequentialAgent\", sub_agents=[agent_a, agent_b]\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(\n",
        "    agent=sequential_agent, app_name=APP_NAME, session_service=session_service\n",
        ")\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "call_agent(\"execute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_26_'></a>[Loop Agent](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_26_1_'></a>[Simple Runner](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Agent A: Starting...\n",
            "\n",
            "Agent Response:  Agent B: Starting...\n",
            "\n",
            "Agent Response:  Agent A: Acknowledged. Agent B is starting. I am ready for further instructions.\n",
            "\n",
            "Agent Response:  Agent B: Acknowledged. Ready for instructions.\n",
            "\n",
            "Agent Response:  Agent A: Acknowledged. Waiting for instructions to be relayed from Agent B or other source.\n",
            "\n",
            "Agent Response:  Agent B: Agent A, please generate five different marketing slogans for a new brand of organic dog treats called \"Happy Paws Bites\". The slogans should be concise, memorable, and appeal to dog owners.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "\n",
        "APP_NAME = \"loop_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"loop_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "agent_a = LlmAgent(\n",
        "    name=\"AgentA\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    instruction=\"You are Agent A. Respond with 'Agent A: Starting...'\",\n",
        ")\n",
        "\n",
        "agent_b = LlmAgent(\n",
        "    name=\"AgentB\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    instruction=\"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        ")\n",
        "\n",
        "# Create the LoopAgent\n",
        "loop_agent = LoopAgent(\n",
        "    name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=3\n",
        ")\n",
        "\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=loop_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "call_agent(\"execute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_26_2_'></a>[With InnvocationContext](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.events import Event, EventActions\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Starting...\")]),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Starting...\")]),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = await session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(\n",
        "        name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=2\n",
        "    )\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(parts=[types.Part(text=\"execute\")]),\n",
        "    )\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_26_3_'></a>[Escalation with Condition](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.events import Event, EventActions\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Starting...\")]),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Starting...\")]),\n",
        "        )\n",
        "\n",
        "        # Example condition: Escalate if session state has a key 'escalate_agent_b'\n",
        "        escalate = ctx.session.state.get(\"escalate_agent_b\", False)\n",
        "\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Finishing...\")]),\n",
        "            actions=EventActions(escalate=escalate),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = await session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(\n",
        "        name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=3\n",
        "    )\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(parts=[types.Part(text=\"execute\")]),\n",
        "    )\n",
        "\n",
        "    # Example: Set a condition to escalate\n",
        "    ctx.session.state[\"escalate_agent_b\"] = True\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_27_'></a>[Parallel Agent](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_27_1_'></a>[Simple](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.parallel_agent import ParallelAgent\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Starting...\")]),\n",
        "        )\n",
        "        await asyncio.sleep(1)\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Starting...\")]),\n",
        "        )\n",
        "        await asyncio.sleep(2)\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = await session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the ParallelAgent\n",
        "    parallel_agent = ParallelAgent(name=\"ParallelAgent\", sub_agents=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=parallel_agent,\n",
        "        user_content=types.Content(parts=[types.Part(text=\"execute\")]),\n",
        "    )\n",
        "\n",
        "    # Run the ParallelAgent\n",
        "    async for event in parallel_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc1_27_2_'></a>[Shared State](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Finishing... Received: Data from Agent A\n"
          ]
        }
      ],
      "source": [
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Starting...\")]),\n",
        "        )\n",
        "        await asyncio.sleep(1)\n",
        "        ctx.session.state[\"shared_data\"] = \"Data from Agent A\"\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent A: Finishing...\")]),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(parts=[types.Part(text=\"Agent B: Starting...\")]),\n",
        "        )\n",
        "        await asyncio.sleep(2)\n",
        "        shared_data = ctx.session.state.get(\"shared_data\", \"No data from Agent A\")\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[\n",
        "                    types.Part(text=f\"Agent B: Finishing... Received: {shared_data}\")\n",
        "                ]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = await session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the ParallelAgent\n",
        "    parallel_agent = ParallelAgent(name=\"ParallelAgent\", sub_agents=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=parallel_agent,\n",
        "        user_content=types.Content(parts=[types.Part(text=\"execute\")]),\n",
        "    )\n",
        "\n",
        "    # Run the ParallelAgent\n",
        "    async for event in parallel_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_28_'></a>[Custom Agent](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb4lAAD4eo77",
        "outputId": "f72e46b3-fcb4-464d-9fda-0e0eee87e240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: Hello from Custom Agent!\n"
          ]
        }
      ],
      "source": [
        "class CustomAgent(BaseAgent):\n",
        "    \"\"\"A custom agent that generates a simple text message.\"\"\"\n",
        "\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            invocation_id=ctx.invocation_id,\n",
        "            author=self.name,\n",
        "            content=types.Content(\n",
        "                parts=[types.Part.from_text(text=\"Hello from Custom Agent!\")],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a custom agent instance\n",
        "    custom_agent = CustomAgent(name=\"CustomAgent\", description=\"A custom agent\")\n",
        "\n",
        "    # Create a session service\n",
        "    session_service = InMemorySessionService()\n",
        "\n",
        "    # Create a session\n",
        "    session = await session_service.create_session(\n",
        "        app_name=\"demo_app\", user_id=\"test_user\", session_id=\"test_session\"\n",
        "    )\n",
        "\n",
        "    # Create a runner instance\n",
        "    runner = Runner(\n",
        "        app_name=\"demo_app\",\n",
        "        agent=custom_agent,\n",
        "        session_service=session_service,\n",
        "    )\n",
        "\n",
        "    # Run the agent\n",
        "    async for event in runner.run_async(\n",
        "        user_id=\"test_user\",\n",
        "        session_id=\"test_session\",\n",
        "        new_message=types.Content(\n",
        "            parts=[types.Part.from_text(text=\"Hi, how are you?\")]\n",
        "        ),\n",
        "    ):\n",
        "        print(f\"Event: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
